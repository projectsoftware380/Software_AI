# src/pipeline/main.py
"""
Pipeline v5 â€“ Final. Reemplaza el filtro RL por un clasificador supervisado (LightGBM).

**Ajustes 2025-06-14**
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1.  Se corrige la llamada al componente **data_ingestion** aÃ±adiendo los argumentos
    requeridos (`project_id`, `polygon_api_key_secret_name`, `end_date`).
2.  Se corrige la llamada al componente **data_preparation** aÃ±adiendo el argumento
    obligatorio `pair` (se usa "ALL" por defecto para procesar el conjunto
    completo de datos antes del bucle paralelo).
3.  No se altera ninguna otra lÃ³gica de la pipeline.

IMPORTANTE â†’ Verifica que `POLYGON_API_KEY_SECRET_NAME` exista en
`src/shared/constants.py` (es el nombre correcto del secreto con la APIâ€‘Key de
Polygon.io).
"""

import argparse
import os
from datetime import datetime
from pathlib import Path

import google.cloud.aiplatform as aip
from kfp import compiler, dsl
from kfp.components import load_component_from_text

from src.shared import constants

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(
    "Compila y/o envÃ­a la pipeline v5 final con filtro supervisado"
)
parser.add_argument(
    "--common-image-uri",
    required=True,
    help="URI Docker â€“ MISMA imagen para **todos** los componentes",
)
args, _ = parser.parse_known_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Infra y utilidades â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
COMPONENTS_DIR = Path(__file__).parent.parent / "components"


def load_utf8_component(rel_path: str):
    """Carga un componente YAML preservando UTF-8 (Windows-safe)."""
    yaml_text = (COMPONENTS_DIR / rel_path).read_text(encoding="utf-8")
    return load_component_from_text(yaml_text)


# Cargar todos los componentes necesarios para la v5
component_op_factory = {
    "data_ingestion": load_utf8_component("data_ingestion/component.yaml"),
    "data_preparation": load_utf8_component("data_preparation/component.yaml"),
    "optimize_model_architecture": load_utf8_component(
        "optimize_model_architecture/component.yaml"
    ),
    "optimize_trading_logic": load_utf8_component(
        "optimize_trading_logic/component.yaml"
    ),
    "train_lstm_launcher": load_utf8_component("train_lstm_launcher/component.yaml"),
    "train_filter_model": load_utf8_component("train_filter_model/component.yaml"),
    "backtest": load_utf8_component("backtest/component.yaml"),
    "model_promotion": load_utf8_component("model_promotion/component.yaml"),
}

# Asignar la misma imagen Docker a todos los contenedores para simplificar
for comp in component_op_factory.values():
    impl = comp.component_spec.implementation
    if hasattr(impl, "container") and impl.container:
        impl.container.image = args.common_image_uri

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DefiniciÃ³n PIPELINE v5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dsl.pipeline(
    name="algo-trading-mlops-pipeline-v5-supervised-filter",
    description=(
        "VersiÃ³n final: HPO Secuencial â†’ LSTM â†’ Filtro LightGBM â†’ Backtest â†’ PromociÃ³n"
    ),
    pipeline_root=constants.PIPELINE_ROOT,
)

def trading_pipeline_v5(
    timeframe: str = constants.DEFAULT_TIMEFRAME,
    n_trials_arch: int = 20,
    n_trials_logic: int = 30,
    backtest_years_to_keep: int = 5,
    holdout_months: int = 3,
):
    """Pipeline de entrenamiento y despliegue para IA de trading Forex."""

    # 1 â–¸ IngestiÃ³n de datos
    ingest_task = component_op_factory["data_ingestion"](
        project_id=constants.PROJECT_ID,
        polygon_secret_name=constants.POLYGON_API_KEY_SECRET_NAME,
        end_date=datetime.utcnow().strftime("%Y-%m-%d"),
        timeframe=timeframe,
    )

    # 2 â–¸ PreparaciÃ³n de datos (con hold-out)
    prepare_opt_data_task = component_op_factory["data_preparation"](
        pair="ALL",  # Procesa todas las divisas para la fase de optimizaciÃ³n
        timeframe=timeframe,
        years_to_keep=backtest_years_to_keep,
        holdout_months=holdout_months,
    ).after(ingest_task)

    # 3 â–¸ Optimizar arquitectura LSTM
    optimize_arch_task = component_op_factory["optimize_model_architecture"](
        features_path=prepare_opt_data_task.outputs["prepared_data_path"],
        n_trials=n_trials_arch,
    )

    # 4 â–¸ Optimizar lÃ³gica de trading (umbrales, hiper-parÃ¡metros, etc.)
    optimize_logic_task = component_op_factory["optimize_trading_logic"](
        features_path=prepare_opt_data_task.outputs["prepared_data_path"],
        architecture_params_dir=optimize_arch_task.outputs["best_architecture_dir"],
        n_trials=n_trials_logic,
    )

    # Recursos recomendados para steps de Optuna
    for task in (optimize_arch_task, optimize_logic_task):
        task.set_cpu_limit("8") \
            .set_memory_limit("30G") \
            .set_accelerator_limit(constants.DEFAULT_VERTEX_GPU_ACCELERATOR_COUNT) \
            .set_accelerator_type(constants.DEFAULT_VERTEX_GPU_ACCELERATOR_TYPE)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bucle paralelo por cada par de divisas â”€â”€â”€â”€â”€â”€â”€â”€â”€
    pairs_to_process = list(constants.SPREADS_PIP.keys())

    with dsl.ParallelFor(
        items=pairs_to_process, name="parallel-training-for-each-pair"
    ) as pair:

        # 5 â–¸ Entrenamiento LSTM para el par actual
        train_lstm_task = component_op_factory["train_lstm_launcher"](
            project_id=constants.PROJECT_ID,
            region=constants.REGION,
            pair=pair,
            timeframe=timeframe,
            params_path=f"{optimize_logic_task.outputs['best_params_dir']}/{pair}/best_params.json",
            features_gcs_path=prepare_opt_data_task.outputs["prepared_data_path"],
            output_gcs_base_dir=constants.LSTM_MODELS_PATH,
            vertex_training_image_uri=args.common_image_uri,
            vertex_machine_type=constants.DEFAULT_VERTEX_GPU_MACHINE_TYPE,
            vertex_accelerator_type=constants.DEFAULT_VERTEX_GPU_ACCELERATOR_TYPE,
            vertex_accelerator_count=constants.DEFAULT_VERTEX_GPU_ACCELERATOR_COUNT,
            vertex_service_account=constants.VERTEX_LSTM_SERVICE_ACCOUNT,
        )

        # 6 â–¸ Entrenamiento del modelo filtro supervisado (LightGBM)
        train_filter_task = component_op_factory["train_filter_model"](
            lstm_model_dir=train_lstm_task.outputs["trained_lstm_dir_path"],
            features_path=prepare_opt_data_task.outputs["prepared_data_path"],
            pair=pair,
            timeframe=timeframe,
            output_gcs_base_dir=constants.FILTER_MODELS_PATH,
        )

        # 7 â–¸ Backtest final sobre el hold-out
        backtest_task = component_op_factory["backtest"](
            lstm_model_dir=train_lstm_task.outputs["trained_lstm_dir_path"],
            filter_model_path=train_filter_task.outputs["trained_filter_model_path"],
            features_path=prepare_opt_data_task.outputs["holdout_data_path"],
            pair=pair,
            timeframe=timeframe,
        )

        # 8 â–¸ PromociÃ³n a producciÃ³n si el backtest pasa los umbrales
        component_op_factory["model_promotion"](
            new_metrics_dir=backtest_task.outputs["output_gcs_dir"],
            new_lstm_artifacts_dir=train_lstm_task.outputs["trained_lstm_dir_path"],
            new_filter_model_path=train_filter_task.outputs["trained_filter_model_path"],
            pair=pair,
            timeframe=timeframe,
            production_base_dir=constants.PRODUCTION_MODELS_PATH,
        ).after(backtest_task)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    PIPELINE_JSON = "algo_trading_mlops_pipeline_v5.json"

    # 1 â–¸ Compilar
    compiler.Compiler().compile(trading_pipeline_v5, PIPELINE_JSON)
    print(f"âœ… Pipeline v5 compilada a {PIPELINE_JSON}")

    # 2 â–¸ Enviar a Vertex AI (si la variable de entorno lo permite)
    if os.getenv("SUBMIT_PIPELINE_TO_VERTEX", "true").lower() == "true":
        aip.init(project=constants.PROJECT_ID, location=constants.REGION)
        display_name = f"algo-trading-v5-supervised-filter-{datetime.utcnow():%Y%m%d-%H%M%S}"
        
        # MODIFICACIÃ“N: Usar aip.PipelineJob y llamar a .run()
        job = aip.PipelineJob(
            display_name=display_name,
            template_path=PIPELINE_JSON,
            pipeline_root=constants.PIPELINE_ROOT,
            enable_caching=True # Habilitar cachÃ© para re-ejecuciones mÃ¡s rÃ¡pidas
        )
        job.run() # Lanza la pipeline en Vertex AI
        print(f"ğŸš€ Pipeline lanzada con Display Name: {display_name}")
    else:
        print("â­ï¸ La pipeline no se enviÃ³ a Vertex AI (SUBMIT_PIPELINE_TO_VERTEX estÃ¡ en 'false').")