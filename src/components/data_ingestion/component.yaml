# src/components/data_ingestion/component.yaml
name: Ingest Market Data
description: Fetches and updates raw OHLC market data from Polygon.io and stores it in GCS as a Parquet file.

# Parámetros de entrada que la pipeline le pasará a este componente.
inputs:
  - name: pair
    type: String
    description: 'The trading pair to ingest, e.g., EURUSD.'
  - name: timeframe
    type: String
    description: 'The timeframe for the data, e.g., 15minute.'
  - name: project_id
    type: String
    description: 'The Google Cloud Project ID where the component is running.'
  - name: polygon_secret_name
    type: String
    description: 'Name of the secret in Secret Manager containing the Polygon API key.'
  - name: start_date
    type: String
    description: 'Start date for data fetching in YYYY-MM-DD format.'
    default: '2010-01-01'
  - name: end_date
    type: String
    description: 'End date for data fetching in YYYY-MM-DD format. Defaults to today.'
  - name: min_rows
    type: Integer
    description: 'Minimum number of rows required for the download to be considered successful.'
    default: 100000

# Parámetros de salida que este componente producirá.
outputs:
  - name: completion_message
    type: String
    description: 'A message indicating the result of the ingestion task.'

# Define cómo se ejecuta el componente.
implementation:
  container:
    # La imagen Docker que contiene todas las dependencias y el código fuente.
    # Esta URI se importa desde src/shared/constants.py en la pipeline principal.
    image: europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest
    
    # El comando para ejecutar la tarea. Se usa 'sh -c' para ejecutar un script de shell.
    command:
      - sh
      - -c
      # El '|' indica que lo siguiente es un script de varias líneas.
      # Primero, se ejecuta la tarea de Python. Si termina con éxito (exit code 0),
      # el operador '&&' permite que se ejecute el comando 'echo'.
      # La salida de 'echo' se redirige (>) al archivo de salida de KFP.
      - |
        python -m src.components.data_ingestion.task \
          --pair "$0" \
          --timeframe "$1" \
          --project-id "$2" \
          --polygon-secret-name "$3" \
          --start-date "$4" \
          --end-date "$5" \
          --min-rows "$6" && \
        echo "Data ingestion completed successfully for pair $0 and timeframe $1" > "$7"
    
    # Los argumentos ahora se pasan como argumentos posicionales ($0, $1, $2, etc.) al script de shell.
    args:
      - {inputValue: pair}
      - {inputValue: timeframe}
      - {inputValue: project_id}
      - {inputValue: polygon_secret_name}
      - {inputValue: start_date}
      - {inputValue: end_date}
      - {inputValue: min_rows}
      - {outputPath: completion_message}