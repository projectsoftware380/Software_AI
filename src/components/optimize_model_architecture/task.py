# src/components/optimize_model_architecture/task.py
"""
Tarea de Optimizaci√≥n de Hiperpar√°metros para la Arquitectura del Modelo. (Versi√≥n con Logging Robusto)

Responsabilidades:
1.  Recibir la ruta a un archivo Parquet de datos preparados para UN SOLO PAR.
2.  Ejecutar un estudio de Optuna para encontrar los mejores hiperpar√°metros.
3.  La m√©trica a optimizar es la p√©rdida de validaci√≥n (`val_loss`).
4.  Guardar el archivo `best_architecture.json` en una ruta GCS versionada.
5.  Limpiar las versiones antiguas de los par√°metros, manteniendo solo la m√°s reciente.
6.  Devolver la ruta base versionada donde se guard√≥ el resultado.
"""
from __future__ import annotations

import argparse
import gc
import json
import logging
import os
import random
import tempfile
import warnings
from datetime import datetime
from pathlib import Path

import numpy as np
import optuna
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from tensorflow.keras import callbacks, layers, models, optimizers

from src.shared import constants, gcs_utils, indicators

# --- Configuraci√≥n del Logging ---
warnings.filterwarnings("ignore", category=FutureWarning)
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

# --- Reproducibilidad y Configuraci√≥n de GPU ---
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
os.environ.setdefault("TF_DETERMINISTIC_OPS", "1")

try:
    gpus = tf.config.list_physical_devices("GPU")
    if gpus:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        tf.keras.mixed_precision.set_global_policy("mixed_float16")
        logger.info("üöÄ GPU(s) detectadas y configuradas para el HPO de arquitectura.")
    else:
        # No lanzamos un error aqu√≠ porque la optimizaci√≥n podr√≠a correr en CPU si es necesario
        logger.warning("‚ö†Ô∏è No se encontr√≥ ninguna GPU. La optimizaci√≥n se ejecutar√° en CPU y ser√° muy lenta.")
except Exception as exc:
    logger.warning("‚ö†Ô∏è No se pudo configurar la GPU (%s). Continuando con CPU.", exc)


# --- Helpers del Modelo y Optuna (L√≥gica Original Intacta) ---
def make_model(inp_shape, lr, dr, filt, units, heads):
    x = inp = layers.Input(shape=inp_shape, dtype=tf.float32)
    x = layers.Conv1D(filt, 3, padding="same", activation="relu")(x)
    x = layers.Bidirectional(layers.LSTM(units, return_sequences=True))(x)
    x = layers.MultiHeadAttention(num_heads=heads, key_dim=units)(x, x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(dr)(x)
    out = layers.Dense(2, dtype="float32")(x)
    model = models.Model(inp, out)
    model.compile(optimizers.Adam(lr), loss="mae")
    return model

def optuna_pruning_callback(trial: optuna.Trial, epoch: int, logs: dict):
    trial.report(logs["val_loss"], step=epoch)
    if trial.should_prune():
        raise optuna.exceptions.TrialPruned()

def to_sequences(mat, up, dn, win):
    X, y_up, y_dn = [], [], []
    for i in range(win, len(mat)):
        X.append(mat[i - win:i])
        y_up.append(up[i])
        y_dn.append(dn[i])
    return np.asarray(X, np.float32), np.asarray(y_up, np.float32), np.asarray(y_dn, np.float32)

# --- Funci√≥n Principal de la Tarea ---
def run_architecture_optimization(
    *,
    features_path: str,
    n_trials: int,
    pair: str,
    output_gcs_dir_base: str,
    best_architecture_dir_output: Path,
    cleanup: bool = True,
) -> None:
    """
    Orquesta el proceso completo de optimizaci√≥n de arquitectura para UN SOLO PAR.
    """
    # [LOG] Punto de control inicial.
    logger.info(f"‚ñ∂Ô∏è Iniciando optimize_model_architecture para el par '{pair}' con {n_trials} trials.")
    logger.info(f"  - Features de entrada: {features_path}")
    logger.info(f"  - Directorio base de salida: {output_gcs_dir_base}")

    try:
        local_features_path = gcs_utils.ensure_gcs_path_and_get_local(features_path)
        df_raw = pd.read_parquet(local_features_path)
        df_raw["timestamp"] = pd.to_datetime(df_raw["timestamp"], unit="ms", errors="coerce")
        logger.info(f"DataFrame cargado para {pair}. Shape: {df_raw.shape}")

        def objective(trial: optuna.Trial) -> float:
            tf.keras.backend.clear_session()
            gc.collect()

            p = {
                "win": trial.suggest_int("win", 20, 60),
                "lr": trial.suggest_float("lr", 1e-4, 3e-3, log=True),
                "dr": trial.suggest_float("dr", 0.1, 0.5),
                "filt": trial.suggest_categorical("filt", [16, 32, 64]),
                "units": trial.suggest_categorical("units", [32, 64, 128]),
                "heads": trial.suggest_categorical("heads", [2, 4, 8]),
                **constants.DUMMY_INDICATOR_PARAMS
            }
            
            # [LOG] Registrar los par√°metros de cada trial.
            logger.debug(f"Trial #{trial.number}: Probando par√°metros {p}")

            df_ind = indicators.build_indicators(df_raw.copy(), p, atr_len=14)
            tick = 0.01 if pair.endswith("JPY") else 0.0001
            horizon = p.get("horizon", 20)
            closes = df_ind.close.values
            atr_vals = df_ind[f"atr_14"].values / tick
            future_prices = np.roll(closes, -horizon)
            future_prices[-horizon:] = np.nan
            price_diffs = (future_prices - closes) / tick
            up_targets = np.maximum(price_diffs, 0) / atr_vals
            dn_targets = np.maximum(-price_diffs, 0) / atr_vals
            valid_mask = ~np.isnan(price_diffs)
            
            feature_cols = [c for c in df_ind.columns if "atr_" not in c and c != "timestamp"]
            X_data = df_ind.loc[valid_mask, feature_cols].select_dtypes(include=np.number).astype(np.float32)
            
            scaler = RobustScaler()
            X_scaled = scaler.fit_transform(X_data)
            
            X_seq, y_up_seq, y_dn_seq = to_sequences(
                X_scaled, up_targets[valid_mask], dn_targets[valid_mask], p["win"]
            )
            
            if len(X_seq) == 0:
                logger.warning(f"Trial #{trial.number}: No se generaron secuencias, podando trial.")
                raise optuna.exceptions.TrialPruned()

            X_train, X_val, y_up_train, y_up_val, y_dn_train, y_dn_val = train_test_split(
                X_seq, y_up_seq, y_dn_seq, test_size=0.2, shuffle=False
            )

            model = make_model(X_train.shape[1:], p["lr"], p["dr"], p["filt"], p["units"], p["heads"])
            
            history = model.fit(
                X_train, np.vstack((y_up_train, y_dn_train)).T,
                validation_data=(X_val, np.vstack((y_up_val, y_dn_val)).T),
                epochs=40,
                batch_size=128,
                verbose=0,
                callbacks=[
                    callbacks.EarlyStopping(patience=5, restore_best_weights=True),
                    callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: optuna_pruning_callback(trial, epoch, logs))
                ]
            )
            
            val_loss = min(history.history['val_loss'])
            logger.debug(f"Trial #{trial.number} finalizado con val_loss: {val_loss:.5f}")
            return val_loss

        logger.info(f"Iniciando estudio de Optuna para {pair}...")
        study = optuna.create_study(
            direction="minimize",
            sampler=optuna.samplers.TPESampler(seed=SEED),
            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5, interval_steps=1)
        )
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
        logger.info(f"Estudio de Optuna para {pair} completado.")

        best_architecture_params = study.best_params.copy()
        best_architecture_params["best_val_loss"] = study.best_value

        # [LOG] Registrar el resultado final de la optimizaci√≥n.
        logger.info(f"Mejor arquitectura encontrada para {pair}. Val_loss: {study.best_value:.5f}")
        logger.info(f"Mejores par√°metros: {json.dumps(best_architecture_params, indent=2)}")

        pair_output_gcs_path = f"{output_gcs_dir_base}/best_architecture.json"
        
        with tempfile.TemporaryDirectory() as tmpdir:
            tmp_json = Path(tmpdir) / "best_architecture.json"
            tmp_json.write_text(json.dumps(best_architecture_params, indent=2))
            logger.info(f"Guardando mejores par√°metros en: {pair_output_gcs_path}")
            gcs_utils.upload_gcs_file(tmp_json, pair_output_gcs_path)
            gcs_utils.verify_gcs_file_exists(pair_output_gcs_path)

        if cleanup:
            base_cleanup_path = f"{constants.ARCHITECTURE_PARAMS_PATH}/{pair}"
            logger.info(f"Iniciando limpieza de versiones antiguas en: {base_cleanup_path}")
            gcs_utils.keep_only_latest_version(base_cleanup_path)

        best_architecture_dir_output.parent.mkdir(parents=True, exist_ok=True)
        best_architecture_dir_output.write_text(output_gcs_dir_base)
        logger.info(f"Ruta de salida '{output_gcs_dir_base}' escrita para KFP.")

    except Exception as e:
        # [LOG] Captura de error fatal.
        logger.critical(f"‚ùå Fallo fatal en optimize_model_architecture para el par '{pair}'. Error: {e}", exc_info=True)
        raise

    logger.info(f"üèÅ Componente optimize_model_architecture para '{pair}' completado exitosamente.")


# --- Punto de Entrada para Ejecuci√≥n como Script ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser("Task de Optimizaci√≥n de Arquitectura de Modelo")
    parser.add_argument("--features-path", required=True)
    parser.add_argument("--n-trials", type=int, default=20)
    parser.add_argument("--pair", required=True)
    parser.add_argument("--cleanup", type=lambda x: (str(x).lower() == 'true'), default=True)
    parser.add_argument("--best-architecture-dir-output", type=Path, required=True)
    
    args = parser.parse_args()
    
    # [LOG] Registro de los argumentos recibidos.
    logger.info("Componente 'optimize_model_architecture' iniciado con los siguientes argumentos:")
    for key, value in vars(args).items():
        logger.info(f"  - {key}: {value}")
        
    ts = datetime.utcnow().strftime("%Y%m%d%H%M%S")
    output_dir_gcs = f"{constants.ARCHITECTURE_PARAMS_PATH}/{args.pair}/{ts}"

    run_architecture_optimization(
        features_path=args.features_path,
        n_trials=args.n_trials,
        pair=args.pair,
        output_gcs_dir_base=output_dir_gcs,
        best_architecture_dir_output=args.best_architecture_dir_output,
        cleanup=args.cleanup,
    )