# src/components/train_rl/component.yaml
name: Train Reinforcement Learning Agent (PPO)
description: Trains a PPO agent from stable-baselines3 to act as a filter for the LSTM model's trading signals.

inputs:
  - name: params_path
    type: String
    description: 'The GCS path to the best_params.json file, which contains the PPO hyperparameters.'
  - name: rl_data_path
    type: String
    description: 'The GCS path to the .npz file containing the observations and raw signals for RL training.'
  - name: pair
    type: String
    description: 'The trading pair for which the agent is being trained.'
  - name: timeframe
    type: String
    description: 'The timeframe for which the agent is being trained.'
  - name: output_gcs_base_dir
    type: String
    description: 'Base GCS directory where the trained RL model (.zip) will be saved in a versioned folder.'
  - name: tensorboard_logs_base_dir
    type: String
    description: 'Base GCS directory for saving TensorBoard logs.'

outputs:
  - name: trained_rl_model_path
    type: String
    description: 'The full GCS path to the trained and saved PPO model .zip file.'

implementation:
  container:
    image: europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest
    command:
      - python
      - -m
      - src.components.train_rl.task
    args:
      - --params-path
      - {inputValue: params_path}
      - --rl-data-path
      - {inputValue: rl_data_path}
      - --pair
      - {inputValue: pair}
      - --timeframe
      - {inputValue: timeframe}
      - --output-gcs-base-dir
      - {inputValue: output_gcs_base_dir}
      - --tensorboard-logs-base-dir
      - {inputValue: tensorboard_logs_base_dir}
      - --trained-rl-model-path-output
      - {outputPath: trained_rl_model_path}