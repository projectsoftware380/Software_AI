{
  "components": {
    "comp-backtest-op": {
      "executorLabel": "exec-backtest-op",
      "inputDefinitions": {
        "parameters": {
          "features_path": {
            "parameterType": "STRING"
          },
          "lstm_model_dir": {
            "parameterType": "STRING"
          },
          "output_gcs_prefix": {
            "parameterType": "STRING"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "rl_model_path": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "backtest_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-condition-2": {
      "dag": {
        "tasks": {
          "notify-pipeline-status-op-2": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-notify-pipeline-status-op-2"
            },
            "inputs": {
              "parameters": {
                "model_promoted": {
                  "runtimeValue": {
                    "constant": true
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "pipeline_job_id": {
                  "runtimeValue": {
                    "constant": "{{$.pipeline_job_uuid}}"
                  }
                },
                "pipeline_job_link": {
                  "runtimeValue": {
                    "constant": "https://console.cloud.google.com/vertex-ai/pipelines/runs/{{$.pipeline_job_uuid}}?project=trading-ai-460823"
                  }
                },
                "pipeline_run_status": {
                  "runtimeValue": {
                    "constant": "SUCCEEDED"
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Notify_Pipeline_Success_Promoted"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "pipelinechannel--decide-promotion-op-Output": {
            "parameterType": "BOOLEAN"
          },
          "pipelinechannel--pair": {
            "parameterType": "STRING"
          },
          "pipelinechannel--timeframe": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-condition-3": {
      "dag": {
        "tasks": {
          "notify-pipeline-status-op-3": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-notify-pipeline-status-op-3"
            },
            "inputs": {
              "parameters": {
                "model_promoted": {
                  "runtimeValue": {
                    "constant": false
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "pipeline_job_id": {
                  "runtimeValue": {
                    "constant": "{{$.pipeline_job_uuid}}"
                  }
                },
                "pipeline_job_link": {
                  "runtimeValue": {
                    "constant": "https://console.cloud.google.com/vertex-ai/pipelines/runs/{{$.pipeline_job_uuid}}?project=trading-ai-460823"
                  }
                },
                "pipeline_run_status": {
                  "runtimeValue": {
                    "constant": "SUCCEEDED"
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Notify_Pipeline_Success_Not_Promoted"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "pipelinechannel--decide-promotion-op-Output": {
            "parameterType": "BOOLEAN"
          },
          "pipelinechannel--pair": {
            "parameterType": "STRING"
          },
          "pipelinechannel--timeframe": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-decide-promotion-op": {
      "executorLabel": "exec-decide-promotion-op",
      "inputDefinitions": {
        "parameters": {
          "current_production_metrics_path": {
            "parameterType": "STRING"
          },
          "gcs_bucket_name": {
            "parameterType": "STRING"
          },
          "new_backtest_metrics_dir": {
            "parameterType": "STRING"
          },
          "new_lstm_artifacts_dir": {
            "parameterType": "STRING"
          },
          "new_rl_model_path": {
            "parameterType": "STRING"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "production_base_dir": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "BOOLEAN"
          }
        }
      }
    },
    "comp-exit-handler-1": {
      "dag": {
        "tasks": {
          "backtest-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-backtest-op"
            },
            "dependentTasks": [
              "train-lstm-vertex-ai-op",
              "train-rl-op"
            ],
            "inputs": {
              "parameters": {
                "features_path": {
                  "componentInputParameter": "pipelinechannel--backtest_features_path"
                },
                "lstm_model_dir": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "train-lstm-vertex-ai-op"
                  }
                },
                "output_gcs_prefix": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/backtest_results_v2"
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "rl_model_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "train-rl-op"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Execute_Full_Backtesting"
            }
          },
          "condition-2": {
            "componentRef": {
              "name": "comp-condition-2"
            },
            "dependentTasks": [
              "decide-promotion-op"
            ],
            "inputs": {
              "parameters": {
                "pipelinechannel--decide-promotion-op-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "decide-promotion-op"
                  }
                },
                "pipelinechannel--pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "pipelinechannel--timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "If_Model_Promoted"
            },
            "triggerPolicy": {
              "condition": "inputs.parameter_values['pipelinechannel--decide-promotion-op-Output'] == true"
            }
          },
          "condition-3": {
            "componentRef": {
              "name": "comp-condition-3"
            },
            "dependentTasks": [
              "decide-promotion-op"
            ],
            "inputs": {
              "parameters": {
                "pipelinechannel--decide-promotion-op-Output": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "decide-promotion-op"
                  }
                },
                "pipelinechannel--pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "pipelinechannel--timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "If_Model_Not_Promoted"
            },
            "triggerPolicy": {
              "condition": "inputs.parameter_values['pipelinechannel--decide-promotion-op-Output'] == false"
            }
          },
          "decide-promotion-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-decide-promotion-op"
            },
            "dependentTasks": [
              "backtest-op",
              "train-lstm-vertex-ai-op",
              "train-rl-op"
            ],
            "inputs": {
              "parameters": {
                "current_production_metrics_path": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/models/production_v2/{{$.inputs.parameters['pipelinechannel--pair']}}/{{$.inputs.parameters['pipelinechannel--timeframe']}}/metrics_production.json"
                  }
                },
                "gcs_bucket_name": {
                  "runtimeValue": {
                    "constant": "trading-ai-models-460823"
                  }
                },
                "new_backtest_metrics_dir": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "backtest-op"
                  }
                },
                "new_lstm_artifacts_dir": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "train-lstm-vertex-ai-op"
                  }
                },
                "new_rl_model_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "train-rl-op"
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "pipelinechannel--pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "pipelinechannel--timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                },
                "production_base_dir": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/models/production_v2"
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Decide_Model_Promotion"
            }
          },
          "optimize-lstm-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-optimize-lstm-op"
            },
            "dependentTasks": [
              "prepare-opt-data-op"
            ],
            "inputs": {
              "parameters": {
                "features_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "prepare-opt-data-op"
                  }
                },
                "gcs_bucket_name": {
                  "runtimeValue": {
                    "constant": "trading-ai-models-460823"
                  }
                },
                "n_trials": {
                  "componentInputParameter": "pipelinechannel--n_trials"
                },
                "output_gcs_prefix": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/params/LSTM_v2"
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Optimize_LSTM_Hyperparameters"
            }
          },
          "prepare-opt-data-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-prepare-opt-data-op"
            },
            "dependentTasks": [
              "update-data-op"
            ],
            "inputs": {
              "parameters": {
                "gcs_bucket_name": {
                  "runtimeValue": {
                    "constant": "trading-ai-models-460823"
                  }
                },
                "output_gcs_prefix": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/data_filtered_for_opt_v2"
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Prepare_Optimization_Data"
            }
          },
          "prepare-rl-data-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-prepare-rl-data-op"
            },
            "dependentTasks": [
              "train-lstm-vertex-ai-op"
            ],
            "inputs": {
              "parameters": {
                "gcs_bucket_name": {
                  "runtimeValue": {
                    "constant": "trading-ai-models-460823"
                  }
                },
                "lstm_model_dir": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "train-lstm-vertex-ai-op"
                  }
                },
                "output_gcs_prefix": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/data_rl_inputs_v2"
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Prepare_RL_Data"
            }
          },
          "train-lstm-vertex-ai-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-lstm-vertex-ai-op"
            },
            "dependentTasks": [
              "optimize-lstm-op"
            ],
            "inputs": {
              "parameters": {
                "gcs_bucket_name": {
                  "runtimeValue": {
                    "constant": "trading-ai-models-460823"
                  }
                },
                "output_gcs_prefix": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/models/LSTM_v2"
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "params_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "optimize-lstm-op"
                  }
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "region": {
                  "runtimeValue": {
                    "constant": "us-central1"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                },
                "vertex_accelerator_count": {
                  "componentInputParameter": "pipelinechannel--vertex_lstm_accelerator_count"
                },
                "vertex_accelerator_type": {
                  "componentInputParameter": "pipelinechannel--vertex_lstm_accelerator_type"
                },
                "vertex_machine_type": {
                  "componentInputParameter": "pipelinechannel--vertex_lstm_machine_type"
                },
                "vertex_service_account": {
                  "componentInputParameter": "pipelinechannel--vertex_lstm_service_account"
                },
                "vertex_training_image_uri": {
                  "componentInputParameter": "pipelinechannel--vertex_lstm_training_image"
                }
              }
            },
            "taskInfo": {
              "name": "Train_LSTM_Model_Vertex_AI"
            }
          },
          "train-rl-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-rl-op"
            },
            "dependentTasks": [
              "prepare-rl-data-op",
              "train-lstm-vertex-ai-op"
            ],
            "inputs": {
              "parameters": {
                "gcs_bucket_name": {
                  "runtimeValue": {
                    "constant": "trading-ai-models-460823"
                  }
                },
                "lstm_model_dir": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "train-lstm-vertex-ai-op"
                  }
                },
                "output_gcs_prefix": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/models/RL_v2"
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "rl_data_path": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "prepare-rl-data-op"
                  }
                },
                "tensorboard_logs_gcs_prefix": {
                  "runtimeValue": {
                    "constant": "gs://trading-ai-models-460823/tensorboard_logs_v2"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Train_PPO_Agent"
            }
          },
          "update-data-op": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-update-data-op"
            },
            "inputs": {
              "parameters": {
                "gcs_bucket_name": {
                  "runtimeValue": {
                    "constant": "trading-ai-models-460823"
                  }
                },
                "pair": {
                  "componentInputParameter": "pipelinechannel--pair"
                },
                "polygon_api_key_secret_name": {
                  "componentInputParameter": "pipelinechannel--polygon_api_key_secret_name_param"
                },
                "polygon_api_key_secret_version": {
                  "componentInputParameter": "pipelinechannel--polygon_api_key_secret_version_param"
                },
                "project_id": {
                  "runtimeValue": {
                    "constant": "trading-ai-460823"
                  }
                },
                "timeframe": {
                  "componentInputParameter": "pipelinechannel--timeframe"
                }
              }
            },
            "taskInfo": {
              "name": "Update_Market_Data"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "pipelinechannel--backtest_features_path": {
            "parameterType": "STRING"
          },
          "pipelinechannel--n_trials": {
            "parameterType": "NUMBER_INTEGER"
          },
          "pipelinechannel--pair": {
            "parameterType": "STRING"
          },
          "pipelinechannel--polygon_api_key_secret_name_param": {
            "parameterType": "STRING"
          },
          "pipelinechannel--polygon_api_key_secret_version_param": {
            "parameterType": "STRING"
          },
          "pipelinechannel--timeframe": {
            "parameterType": "STRING"
          },
          "pipelinechannel--vertex_lstm_accelerator_count": {
            "parameterType": "NUMBER_INTEGER"
          },
          "pipelinechannel--vertex_lstm_accelerator_type": {
            "parameterType": "STRING"
          },
          "pipelinechannel--vertex_lstm_machine_type": {
            "parameterType": "STRING"
          },
          "pipelinechannel--vertex_lstm_service_account": {
            "parameterType": "STRING"
          },
          "pipelinechannel--vertex_lstm_training_image": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-notify-pipeline-status-op": {
      "executorLabel": "exec-notify-pipeline-status-op",
      "inputDefinitions": {
        "parameters": {
          "model_promoted": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "pipeline_job_id": {
            "parameterType": "STRING"
          },
          "pipeline_job_link": {
            "parameterType": "STRING"
          },
          "pipeline_run_status": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "BOOLEAN"
          }
        }
      }
    },
    "comp-notify-pipeline-status-op-2": {
      "executorLabel": "exec-notify-pipeline-status-op-2",
      "inputDefinitions": {
        "parameters": {
          "model_promoted": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "pipeline_job_id": {
            "parameterType": "STRING"
          },
          "pipeline_job_link": {
            "parameterType": "STRING"
          },
          "pipeline_run_status": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "BOOLEAN"
          }
        }
      }
    },
    "comp-notify-pipeline-status-op-3": {
      "executorLabel": "exec-notify-pipeline-status-op-3",
      "inputDefinitions": {
        "parameters": {
          "model_promoted": {
            "defaultValue": false,
            "isOptional": true,
            "parameterType": "BOOLEAN"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "pipeline_job_id": {
            "parameterType": "STRING"
          },
          "pipeline_job_link": {
            "parameterType": "STRING"
          },
          "pipeline_run_status": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "BOOLEAN"
          }
        }
      }
    },
    "comp-optimize-lstm-op": {
      "executorLabel": "exec-optimize-lstm-op",
      "inputDefinitions": {
        "parameters": {
          "features_path": {
            "parameterType": "STRING"
          },
          "gcs_bucket_name": {
            "parameterType": "STRING"
          },
          "n_trials": {
            "parameterType": "NUMBER_INTEGER"
          },
          "output_gcs_prefix": {
            "parameterType": "STRING"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "optimization_metrics": {
            "artifactType": {
              "schemaTitle": "system.Metrics",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-prepare-opt-data-op": {
      "executorLabel": "exec-prepare-opt-data-op",
      "inputDefinitions": {
        "parameters": {
          "gcs_bucket_name": {
            "parameterType": "STRING"
          },
          "output_gcs_prefix": {
            "parameterType": "STRING"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-prepare-rl-data-op": {
      "executorLabel": "exec-prepare-rl-data-op",
      "inputDefinitions": {
        "parameters": {
          "gcs_bucket_name": {
            "parameterType": "STRING"
          },
          "lstm_model_dir": {
            "parameterType": "STRING"
          },
          "output_gcs_prefix": {
            "parameterType": "STRING"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-train-lstm-vertex-ai-op": {
      "executorLabel": "exec-train-lstm-vertex-ai-op",
      "inputDefinitions": {
        "parameters": {
          "gcs_bucket_name": {
            "parameterType": "STRING"
          },
          "output_gcs_prefix": {
            "parameterType": "STRING"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "params_path": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "region": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          },
          "vertex_accelerator_count": {
            "parameterType": "NUMBER_INTEGER"
          },
          "vertex_accelerator_type": {
            "parameterType": "STRING"
          },
          "vertex_machine_type": {
            "parameterType": "STRING"
          },
          "vertex_service_account": {
            "parameterType": "STRING"
          },
          "vertex_training_image_uri": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-train-rl-op": {
      "executorLabel": "exec-train-rl-op",
      "inputDefinitions": {
        "parameters": {
          "gcs_bucket_name": {
            "parameterType": "STRING"
          },
          "lstm_model_dir": {
            "parameterType": "STRING"
          },
          "output_gcs_prefix": {
            "parameterType": "STRING"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "rl_data_path": {
            "parameterType": "STRING"
          },
          "tensorboard_logs_gcs_prefix": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-update-data-op": {
      "executorLabel": "exec-update-data-op",
      "inputDefinitions": {
        "parameters": {
          "gcs_bucket_name": {
            "parameterType": "STRING"
          },
          "pair": {
            "parameterType": "STRING"
          },
          "polygon_api_key_secret_name": {
            "parameterType": "STRING"
          },
          "polygon_api_key_secret_version": {
            "defaultValue": "latest",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "timeframe": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "Output": {
            "parameterType": "STRING"
          }
        }
      }
    }
  },
  "defaultPipelineRoot": "gs://trading-ai-models-460823/pipeline_root_v2",
  "deploymentSpec": {
    "executors": {
      "exec-backtest-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "backtest_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'gcsfs' 'pandas' 'numpy' 'tensorflow' 'stable-baselines3' 'gymnasium' 'pandas_ta' 'scikit-learn' 'joblib' 'optuna' 'kfp-pipeline-spec>=0.1.16' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef backtest_op(\n    lstm_model_dir: str, # Versioned GCS path from train_lstm_vertex_ai_op\n    rl_model_path: str,  # GCS path to the RL model zip file from train_rl_op\n    features_path: str,  # GCS path to the unseen backtest data\n    pair: str,\n    timeframe: str,\n    output_gcs_prefix: str, # Base GCS path for backtest outputs\n    project_id: str,\n    backtest_metrics: Output[Metrics]\n) -> str: # Returns GCS path to the directory containing backtest results (including metrics.json)\n    import subprocess\n    from datetime import datetime\n    import logging\n    import json as py_json\n    from google.cloud import storage as gcp_storage\n    from pathlib import Path\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    if not lstm_model_dir.startswith(\"gs://\") or not rl_model_path.startswith(\"gs://\"):\n        raise ValueError(\"backtest_op requiere rutas GCS v\u00e1lidas para los modelos.\")\n\n    # Construct paths to individual files within the versioned LSTM model directory\n    lstm_model_file_path = f\"{lstm_model_dir.rstrip('/')}/model.h5\"\n    lstm_scaler_path = f\"{lstm_model_dir.rstrip('/')}/scaler.pkl\"\n    lstm_params_path = f\"{lstm_model_dir.rstrip('/')}/params.json\" # The one copied by train_lstm.py\n\n    timestamp_str = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n    # Backtest script will save its outputs (including metrics.json) into this versioned directory\n    backtest_output_gcs_dir = f\"{output_gcs_prefix.rstrip('/')}/{pair}/{timeframe}/{timestamp_str}/\"\n    expected_metrics_file_gcs_path = f\"{backtest_output_gcs_dir.rstrip('/')}/metrics.json\"\n\n    logger.info(f\"Initializing backtest.py for {pair} {timeframe}. Output dir: {backtest_output_gcs_dir}\")\n    command = [\n        \"python\", \"backtest.py\",\n        \"--pair\", pair,\n        \"--timeframe\", timeframe,\n        \"--lstm-model-path\", lstm_model_file_path,\n        \"--lstm-scaler-path\", lstm_scaler_path,\n        \"--lstm-params-path\", lstm_params_path, # Params from LSTM artifact dir\n        \"--rl-model-path\", rl_model_path,       # Path to the RL .zip model\n        \"--features-path\", features_path,       # Unseen data for backtesting\n        \"--output-dir\", backtest_output_gcs_dir, # Script will save all outputs here\n    ]\n    logger.info(f\"Executing command: {' '.join(command)}\")\n    process = subprocess.run(command, capture_output=True, text=True, check=False)\n\n    if process.returncode != 0:\n        error_msg = f\"Error in backtest.py for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\\nSTDERR: {process.stderr}\"\n        logger.error(error_msg)\n        raise RuntimeError(error_msg)\n    else:\n        logger.info(f\"backtest.py completed for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\")\n        try:\n            storage_client = gcp_storage.Client(project=project_id)\n            path_parts = expected_metrics_file_gcs_path.replace(\"gs://\", \"\").split(\"/\", 1)\n            bucket_name_str = path_parts[0]\n            blob_path_str = path_parts[1]\n\n            bucket = storage_client.bucket(bucket_name_str)\n            blob = bucket.blob(blob_path_str)\n\n            if blob.exists():\n                logger.info(f\"Attempting to download metrics from: {expected_metrics_file_gcs_path}\")\n                metrics_content = blob.download_as_string()\n                metrics_data = py_json.loads(metrics_content)\n                logger.info(f\"Metrics data loaded: {metrics_data}\")\n\n                metrics_to_log = metrics_data.get(\"filtered\", metrics_data)\n\n                for key, value in metrics_to_log.items():\n                    if isinstance(value, (int, float)):\n                        backtest_metrics.log_metric(key, value)\n                    else:\n                        try:\n                            float_value = float(value)\n                            backtest_metrics.log_metric(key, float_value)\n                        except (ValueError, TypeError):\n                            logger.warning(f\"Cannot log metric '{key}' with value '{value}' (type: {type(value)}) as float. Skipping.\")\n                logger.info(f\"Logged metrics from {expected_metrics_file_gcs_path} (section 'filtered' or root) to KFP Metrics.\")\n            else:\n                logger.warning(f\"Metrics file {expected_metrics_file_gcs_path} not found for KFP logging. STDOUT from backtest.py might contain clues:\\n{process.stdout}\")\n        except Exception as e_metrics:\n            logger.error(f\"Failed to log metrics from {expected_metrics_file_gcs_path} to KFP: {e_metrics}\", exc_info=True)\n    return backtest_output_gcs_dir.rstrip('/') # Return the GCS path to the directory\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest",
          "resources": {
            "cpuLimit": 8.0,
            "memoryLimit": 30.0,
            "resourceCpuLimit": "8",
            "resourceMemoryLimit": "30G"
          }
        }
      },
      "exec-decide-promotion-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "decide_promotion_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'gcsfs' 'pandas' 'numpy' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef decide_promotion_op(\n    new_backtest_metrics_dir: str, # GCS path to directory from backtest_op\n    new_lstm_artifacts_dir: str, # GCS path to versioned LSTM model dir from train_lstm_vertex_ai_op\n    new_rl_model_path: str,      # GCS path to RL model .zip file from train_rl_op\n    gcs_bucket_name: str,\n    pair: str,\n    timeframe: str,\n    project_id: str,\n    current_production_metrics_path: str, # GCS path to current prod metrics.json\n    production_base_dir: str, # Base GCS path for production models (e.g., gs://<bucket>/models/production_v2)\n) -> bool: # Returns True if model promoted, False otherwise\n    import subprocess\n    import logging\n    from pathlib import Path\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    # new_backtest_metrics_dir is the directory containing metrics.json\n    new_metrics_file_path = f\"{new_backtest_metrics_dir.rstrip('/')}/metrics.json\"\n\n    if not new_lstm_artifacts_dir.startswith(\"gs://\") or \\\n       not new_rl_model_path.startswith(\"gs://\") or \\\n       not new_metrics_file_path.startswith(\"gs://\"): # Check the derived metrics file path\n        raise ValueError(\"decide_promotion_op requiere rutas GCS v\u00e1lidas para artefactos y m\u00e9tricas.\")\n\n    # Target directory for promotion, e.g., gs://<bucket>/models/production_v2/<pair>/<timeframe>/\n    production_pair_timeframe_dir = f\"{production_base_dir.rstrip('/')}/{pair}/{timeframe}\"\n\n    logger.info(f\"Initializing model promotion decision for {pair} {timeframe}.\")\n    logger.info(f\"  New metrics file: {new_metrics_file_path}\")\n    logger.info(f\"  Current prod metrics: {current_production_metrics_path}\")\n    logger.info(f\"  New LSTM dir (source for promotion): {new_lstm_artifacts_dir}\")\n    logger.info(f\"  New RL model (source for promotion): {new_rl_model_path}\")\n    logger.info(f\"  Production target dir (destination for promotion): {production_pair_timeframe_dir}\")\n\n    command = [\n        \"python\", \"model_promotion_decision.py\",\n        \"--new-metrics-path\", new_metrics_file_path,\n        \"--current-production-metrics-path\", current_production_metrics_path,\n        \"--new-lstm-artifacts-dir\", new_lstm_artifacts_dir, # Pass the whole dir\n        \"--new-rl-model-path\", new_rl_model_path,         # Pass the .zip model path\n        \"--production-pair-timeframe-dir\", production_pair_timeframe_dir, # Target dir for copying\n        \"--pair\", pair,\n        \"--timeframe\", timeframe,\n        # --project-id and --gcs-bucket-name might be needed if model_promotion_decision.py interacts with GCS directly\n        # Add them here if required by the script. For now, assuming it primarily uses paths.\n    ]\n    logger.info(f\"Executing command: {' '.join(command)}\")\n    process = subprocess.run(command, capture_output=True, text=True, check=False)\n\n    model_promoted_flag = False\n    if process.returncode == 0:\n        logger.info(f\"model_promotion_decision.py completed.\\nSTDOUT: {process.stdout}\")\n        # Check stdout for a clear signal of promotion.\n        if \"PROMOVIDO a producci\u00f3n.\" in process.stdout or \"Model promoted to production\" in process.stdout.lower():\n            model_promoted_flag = True\n            logger.info(\"Model promotion confirmed based on script output.\")\n        else:\n            logger.info(\"Model not promoted based on script output.\")\n    else:\n        # If the script fails, assume no promotion.\n        logger.error(f\"Error in model_promotion_decision.py. Model will not be promoted.\\nSTDOUT: {process.stdout}\\nSTDERR: {process.stderr}\")\n\n    logger.info(f\"Model promoted: {model_promoted_flag}\")\n    return model_promoted_flag\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest",
          "resources": {
            "cpuLimit": 2.0,
            "memoryLimit": 4.0,
            "resourceCpuLimit": "2",
            "resourceMemoryLimit": "4G"
          }
        }
      },
      "exec-notify-pipeline-status-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "notify_pipeline_status_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-pubsub' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef notify_pipeline_status_op(\n    project_id: str,\n    pair: str,\n    timeframe: str,\n    pipeline_run_status: str, # This will be set by KFP ExitHandler or dsl.If conditions\n    pipeline_job_id: str,\n    pipeline_job_link: str,\n    model_promoted: bool = False, # Default to False, set to True if promotion occurs\n) -> bool:\n    from google.cloud import pubsub_v1\n    import logging\n    import json as py_json\n    from datetime import datetime, timezone\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    SUCCESS_TOPIC_ID = \"data-ingestion-success\" # Or a more generic pipeline success topic\n    FAILURE_TOPIC_ID = \"data-ingestion-failures\" # Or a more generic pipeline failure topic\n\n    try:\n        publisher = pubsub_v1.PublisherClient()\n        ts_utc_iso = datetime.now(timezone.utc).isoformat()\n\n        # Normalize status for KFP v2 values if needed\n        status_summary = pipeline_run_status.upper()\n        # KFP v2 uses \"SUCCEEDED\", \"FAILED\", \"CANCELLED\"\n        # The ExitHandler might pass dsl.PIPELINE_STATUS_PLACEHOLDER which resolves to these.\n        if status_summary not in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\", \"UNKNOWN_STATUS\"]: # UNKNOWN_STATUS is my placeholder for ExitHandler\n            logger.warning(f\"Pipeline status '{pipeline_run_status}' is not one of the expected canonical KFP v2 statuses (SUCCEEDED, FAILED, CANCELLED) or UNKNOWN_STATUS. Will attempt to normalize.\")\n            if \"fail\" in pipeline_run_status.lower() or \"error\" in pipeline_run_status.lower():\n                status_summary = \"FAILED\"\n            elif \"success\" in pipeline_run_status.lower() or \"succeeded\" in pipeline_run_status.lower():\n                status_summary = \"SUCCEEDED\"\n            else: # If it's something else from a direct call, treat as unknown/failure for notification\n                status_summary = \"UNKNOWN_STATUS\" # This will route to failure topic\n\n        details = {\n            \"pipeline_name\": \"algo-trading-mlops-gcp-pipeline-v2\",\n            \"pipeline_job_id\": pipeline_job_id,\n            \"pipeline_job_link\": pipeline_job_link,\n            \"pair\": pair,\n            \"timeframe\": timeframe,\n            \"run_status\": status_summary, # Use the (potentially normalized) status\n            \"model_promoted_status\": model_promoted if status_summary == \"SUCCEEDED\" else False, # Only relevant on success\n            \"timestamp_utc\": ts_utc_iso,\n        }\n\n        target_topic_id = SUCCESS_TOPIC_ID if status_summary == \"SUCCEEDED\" else FAILURE_TOPIC_ID\n        if status_summary == \"UNKNOWN_STATUS\": # UNKNOWN_STATUS from ExitHandler default should go to failure\n             target_topic_id = FAILURE_TOPIC_ID\n\n        summary_message = f\"MLOps Pipeline Notification: {details['pipeline_name']} for {pair}/{timeframe} finished with status: {status_summary}.\"\n        if status_summary == \"SUCCEEDED\":\n            summary_message += f\" Model Promoted: {model_promoted}.\"\n\n        msg_data = {\"summary\": summary_message, \"details\": details}\n        msg_bytes = py_json.dumps(msg_data, indent=2).encode(\"utf-8\")\n        topic_path = publisher.topic_path(project_id, target_topic_id)\n\n        future = publisher.publish(topic_path, msg_bytes)\n        message_id = future.result(timeout=60) # Wait for publish to complete\n        logger.info(f\"Notification sent to Pub/Sub topic '{topic_path}' with message ID: {message_id}.\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error publishing notification to Pub/Sub: {e}\", exc_info=True)\n        return False # Indicate failure to publish\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest",
          "resources": {
            "cpuLimit": 1.0,
            "memoryLimit": 1.0,
            "resourceCpuLimit": "1",
            "resourceMemoryLimit": "1G"
          }
        }
      },
      "exec-notify-pipeline-status-op-2": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "notify_pipeline_status_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-pubsub' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef notify_pipeline_status_op(\n    project_id: str,\n    pair: str,\n    timeframe: str,\n    pipeline_run_status: str, # This will be set by KFP ExitHandler or dsl.If conditions\n    pipeline_job_id: str,\n    pipeline_job_link: str,\n    model_promoted: bool = False, # Default to False, set to True if promotion occurs\n) -> bool:\n    from google.cloud import pubsub_v1\n    import logging\n    import json as py_json\n    from datetime import datetime, timezone\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    SUCCESS_TOPIC_ID = \"data-ingestion-success\" # Or a more generic pipeline success topic\n    FAILURE_TOPIC_ID = \"data-ingestion-failures\" # Or a more generic pipeline failure topic\n\n    try:\n        publisher = pubsub_v1.PublisherClient()\n        ts_utc_iso = datetime.now(timezone.utc).isoformat()\n\n        # Normalize status for KFP v2 values if needed\n        status_summary = pipeline_run_status.upper()\n        # KFP v2 uses \"SUCCEEDED\", \"FAILED\", \"CANCELLED\"\n        # The ExitHandler might pass dsl.PIPELINE_STATUS_PLACEHOLDER which resolves to these.\n        if status_summary not in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\", \"UNKNOWN_STATUS\"]: # UNKNOWN_STATUS is my placeholder for ExitHandler\n            logger.warning(f\"Pipeline status '{pipeline_run_status}' is not one of the expected canonical KFP v2 statuses (SUCCEEDED, FAILED, CANCELLED) or UNKNOWN_STATUS. Will attempt to normalize.\")\n            if \"fail\" in pipeline_run_status.lower() or \"error\" in pipeline_run_status.lower():\n                status_summary = \"FAILED\"\n            elif \"success\" in pipeline_run_status.lower() or \"succeeded\" in pipeline_run_status.lower():\n                status_summary = \"SUCCEEDED\"\n            else: # If it's something else from a direct call, treat as unknown/failure for notification\n                status_summary = \"UNKNOWN_STATUS\" # This will route to failure topic\n\n        details = {\n            \"pipeline_name\": \"algo-trading-mlops-gcp-pipeline-v2\",\n            \"pipeline_job_id\": pipeline_job_id,\n            \"pipeline_job_link\": pipeline_job_link,\n            \"pair\": pair,\n            \"timeframe\": timeframe,\n            \"run_status\": status_summary, # Use the (potentially normalized) status\n            \"model_promoted_status\": model_promoted if status_summary == \"SUCCEEDED\" else False, # Only relevant on success\n            \"timestamp_utc\": ts_utc_iso,\n        }\n\n        target_topic_id = SUCCESS_TOPIC_ID if status_summary == \"SUCCEEDED\" else FAILURE_TOPIC_ID\n        if status_summary == \"UNKNOWN_STATUS\": # UNKNOWN_STATUS from ExitHandler default should go to failure\n             target_topic_id = FAILURE_TOPIC_ID\n\n        summary_message = f\"MLOps Pipeline Notification: {details['pipeline_name']} for {pair}/{timeframe} finished with status: {status_summary}.\"\n        if status_summary == \"SUCCEEDED\":\n            summary_message += f\" Model Promoted: {model_promoted}.\"\n\n        msg_data = {\"summary\": summary_message, \"details\": details}\n        msg_bytes = py_json.dumps(msg_data, indent=2).encode(\"utf-8\")\n        topic_path = publisher.topic_path(project_id, target_topic_id)\n\n        future = publisher.publish(topic_path, msg_bytes)\n        message_id = future.result(timeout=60) # Wait for publish to complete\n        logger.info(f\"Notification sent to Pub/Sub topic '{topic_path}' with message ID: {message_id}.\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error publishing notification to Pub/Sub: {e}\", exc_info=True)\n        return False # Indicate failure to publish\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest"
        }
      },
      "exec-notify-pipeline-status-op-3": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "notify_pipeline_status_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-pubsub' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef notify_pipeline_status_op(\n    project_id: str,\n    pair: str,\n    timeframe: str,\n    pipeline_run_status: str, # This will be set by KFP ExitHandler or dsl.If conditions\n    pipeline_job_id: str,\n    pipeline_job_link: str,\n    model_promoted: bool = False, # Default to False, set to True if promotion occurs\n) -> bool:\n    from google.cloud import pubsub_v1\n    import logging\n    import json as py_json\n    from datetime import datetime, timezone\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    SUCCESS_TOPIC_ID = \"data-ingestion-success\" # Or a more generic pipeline success topic\n    FAILURE_TOPIC_ID = \"data-ingestion-failures\" # Or a more generic pipeline failure topic\n\n    try:\n        publisher = pubsub_v1.PublisherClient()\n        ts_utc_iso = datetime.now(timezone.utc).isoformat()\n\n        # Normalize status for KFP v2 values if needed\n        status_summary = pipeline_run_status.upper()\n        # KFP v2 uses \"SUCCEEDED\", \"FAILED\", \"CANCELLED\"\n        # The ExitHandler might pass dsl.PIPELINE_STATUS_PLACEHOLDER which resolves to these.\n        if status_summary not in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\", \"UNKNOWN_STATUS\"]: # UNKNOWN_STATUS is my placeholder for ExitHandler\n            logger.warning(f\"Pipeline status '{pipeline_run_status}' is not one of the expected canonical KFP v2 statuses (SUCCEEDED, FAILED, CANCELLED) or UNKNOWN_STATUS. Will attempt to normalize.\")\n            if \"fail\" in pipeline_run_status.lower() or \"error\" in pipeline_run_status.lower():\n                status_summary = \"FAILED\"\n            elif \"success\" in pipeline_run_status.lower() or \"succeeded\" in pipeline_run_status.lower():\n                status_summary = \"SUCCEEDED\"\n            else: # If it's something else from a direct call, treat as unknown/failure for notification\n                status_summary = \"UNKNOWN_STATUS\" # This will route to failure topic\n\n        details = {\n            \"pipeline_name\": \"algo-trading-mlops-gcp-pipeline-v2\",\n            \"pipeline_job_id\": pipeline_job_id,\n            \"pipeline_job_link\": pipeline_job_link,\n            \"pair\": pair,\n            \"timeframe\": timeframe,\n            \"run_status\": status_summary, # Use the (potentially normalized) status\n            \"model_promoted_status\": model_promoted if status_summary == \"SUCCEEDED\" else False, # Only relevant on success\n            \"timestamp_utc\": ts_utc_iso,\n        }\n\n        target_topic_id = SUCCESS_TOPIC_ID if status_summary == \"SUCCEEDED\" else FAILURE_TOPIC_ID\n        if status_summary == \"UNKNOWN_STATUS\": # UNKNOWN_STATUS from ExitHandler default should go to failure\n             target_topic_id = FAILURE_TOPIC_ID\n\n        summary_message = f\"MLOps Pipeline Notification: {details['pipeline_name']} for {pair}/{timeframe} finished with status: {status_summary}.\"\n        if status_summary == \"SUCCEEDED\":\n            summary_message += f\" Model Promoted: {model_promoted}.\"\n\n        msg_data = {\"summary\": summary_message, \"details\": details}\n        msg_bytes = py_json.dumps(msg_data, indent=2).encode(\"utf-8\")\n        topic_path = publisher.topic_path(project_id, target_topic_id)\n\n        future = publisher.publish(topic_path, msg_bytes)\n        message_id = future.result(timeout=60) # Wait for publish to complete\n        logger.info(f\"Notification sent to Pub/Sub topic '{topic_path}' with message ID: {message_id}.\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error publishing notification to Pub/Sub: {e}\", exc_info=True)\n        return False # Indicate failure to publish\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest"
        }
      },
      "exec-optimize-lstm-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "optimize_lstm_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'gcsfs' 'pandas' 'numpy' 'tensorflow' 'optuna' 'scikit-learn' 'joblib' 'kfp-pipeline-spec>=0.1.16' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef optimize_lstm_op(\n    gcs_bucket_name: str,\n    features_path: str,\n    pair: str,\n    timeframe: str,\n    n_trials: int,\n    output_gcs_prefix: str,\n    project_id: str,\n    optimization_metrics: Output[Metrics]\n) -> str:\n    import subprocess\n    from datetime import datetime\n    import json\n    import logging\n    from pathlib import Path\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    timestamp_str = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n    params_output_dir = f\"{output_gcs_prefix.rstrip('/')}/{pair}/{timeframe}/{timestamp_str}\"\n    final_params_gcs_path = f\"{params_output_dir}/best_params.json\"\n\n    logger.info(f\"Initializing optimize_lstm.py for {pair} {timeframe} (Features: {features_path}, Params Output Dir: {params_output_dir}, Final Params File: {final_params_gcs_path}, Trials: {n_trials})\")\n\n    command = [\n        \"python\", \"optimize_lstm.py\",\n        \"--features\", features_path,\n        \"--pair\", pair,\n        \"--timeframe\", timeframe,\n        \"--output\", final_params_gcs_path,\n        \"--n-trials\", str(n_trials),\n    ]\n    logger.info(f\"Executing command: {' '.join(command)}\")\n    process = subprocess.run(command, capture_output=True, text=True, check=False)\n\n    if process.returncode != 0:\n        error_msg = f\"Error in optimize_lstm.py for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\\nSTDERR: {process.stderr}\"\n        logger.error(error_msg)\n        raise RuntimeError(error_msg)\n    else:\n        logger.info(f\"optimize_lstm.py completed for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\")\n        try:\n            best_score_line = None\n            for line in process.stdout.splitlines():\n                if \"Best is trial\" in line and \"with value:\" in line:\n                    best_score_line = line\n                    break\n            if best_score_line:\n                parts = best_score_line.split(\"with value:\")\n                if len(parts) > 1:\n                    score_str = parts[1].split(\" \")[1].replace(\".\", \"\")\n                    score_value_str = ''.join(filter(lambda x: x.isdigit() or x == '.' or x == '-', score_str.split()[0]))\n                    score_value = float(score_value_str)\n                    optimization_metrics.log_metric(\"optuna_best_trial_score\", score_value)\n                    logger.info(f\"Optuna best trial score metric logged: {score_value}\")\n            else:\n                logger.warning(\"Could not find Optuna best trial score in stdout for metrics logging.\")\n        except Exception as e:\n            logger.warning(f\"Error parsing Optuna best trial score: {e}. Metrics may not be fully logged. Full stdout was:\\n{process.stdout}\")\n    return final_params_gcs_path\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest",
          "resources": {
            "cpuLimit": 8.0,
            "memoryLimit": 30.0,
            "resourceCpuLimit": "8",
            "resourceMemoryLimit": "30G"
          }
        }
      },
      "exec-prepare-opt-data-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "prepare_opt_data_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'gcsfs' 'pandas' 'numpy' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef prepare_opt_data_op(\n    gcs_bucket_name: str,\n    pair: str,\n    timeframe: str,\n    output_gcs_prefix: str,\n    project_id: str,\n) -> str:\n    # TODO: Consider parameterizing the date range for data selection (e.g., \"last_n_days\", \"start_date\", \"end_date\")\n    # This would allow more flexibility in defining the dataset for optimization.\n    # Currently, prepare_opt_data.py might have a hardcoded or implicit logic for \"recent\" data.\n    import subprocess\n    from datetime import datetime\n    import logging\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n    input_gcs_path = f\"gs://{gcs_bucket_name}/data/{pair}/{timeframe}/{pair}_{timeframe}.parquet\"\n    timestamp_str = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n    output_gcs_path = f\"{output_gcs_prefix.rstrip('/')}/{pair}/{timeframe}/{timestamp_str}/{pair}_{timeframe}_recent.parquet\"\n    logger.info(f\"Initializing prepare_opt_data.py for {pair} {timeframe} (Input: {input_gcs_path}, Output: {output_gcs_path})\")\n    command = [\"python\", \"prepare_opt_data.py\", \"--input_path\", input_gcs_path, \"--output_path\", output_gcs_path]\n    process = subprocess.run(command, capture_output=True, text=True, check=False)\n\n    if process.returncode != 0:\n        logger.error(f\"Error in prepare_opt_data.py for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\\nSTDERR: {process.stderr}\")\n        raise RuntimeError(f\"prepare_opt_data.py failed for {pair} {timeframe}\")\n    else:\n        logger.info(f\"prepare_opt_data.py completed for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\")\n    return output_gcs_path\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest",
          "resources": {
            "cpuLimit": 4.0,
            "memoryLimit": 15.0,
            "resourceCpuLimit": "4",
            "resourceMemoryLimit": "15G"
          }
        }
      },
      "exec-prepare-rl-data-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "prepare_rl_data_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'gcsfs' 'pandas' 'numpy' 'tensorflow' 'joblib' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef prepare_rl_data_op(\n    gcs_bucket_name: str,\n    lstm_model_dir: str, # This is the versioned GCS path from train_lstm_vertex_ai_op\n    pair: str,\n    timeframe: str,\n    output_gcs_prefix: str,\n    project_id: str,\n) -> str:\n    import subprocess\n    from datetime import datetime\n    import logging\n    from pathlib import Path\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    if not lstm_model_dir or not lstm_model_dir.startswith(\"gs://\"):\n        raise ValueError(f\"prepare_rl_data_op requiere una ruta GCS v\u00e1lida para lstm_model_dir, se obtuvo: {lstm_model_dir}\")\n\n    # lstm_model_dir is already the versioned path like gs://<bucket>/models/LSTM_v2/<pair>/<timeframe>/<timestamp>/\n    lstm_model_path = f\"{lstm_model_dir.rstrip('/')}/model.h5\"\n    lstm_scaler_path = f\"{lstm_model_dir.rstrip('/')}/scaler.pkl\"\n    lstm_params_path = f\"{lstm_model_dir.rstrip('/')}/params.json\" # This params.json is the one COPIED by train_lstm.py into its output\n\n    timestamp_str = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n    # Create a versioned output path for RL data as well\n    rl_data_output_path = f\"{output_gcs_prefix.rstrip('/')}/{pair}/{timeframe}/{timestamp_str}/ppo_input_data.npz\"\n\n    logger.info(f\"Initializing prepare_rl_data.py for {pair} {timeframe}. LSTM model dir: {lstm_model_dir}. Output NPZ: {rl_data_output_path}\")\n    command = [\n        \"python\", \"prepare_rl_data.py\",\n        \"--model\", lstm_model_path,\n        \"--scaler\", lstm_scaler_path,\n        \"--params\", lstm_params_path, # This should be the params.json from the LSTM model's artifact directory\n        \"--output\", rl_data_output_path,\n        \"--pair\", pair,\n        \"--timeframe\", timeframe,\n        \"--gcs-bucket-name\", gcs_bucket_name, # Pass the bucket name for the script to read raw data\n    ]\n    logger.info(f\"Executing command: {' '.join(command)}\")\n    process = subprocess.run(command, capture_output=True, text=True, check=False)\n\n    if process.returncode != 0:\n        error_msg = f\"Error in prepare_rl_data.py for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\\nSTDERR: {process.stderr}\"\n        logger.error(error_msg)\n        raise RuntimeError(error_msg)\n    else:\n        logger.info(f\"prepare_rl_data.py completed for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\")\n    return rl_data_output_path\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest",
          "resources": {
            "cpuLimit": 8.0,
            "memoryLimit": 30.0,
            "resourceCpuLimit": "8",
            "resourceMemoryLimit": "30G"
          }
        }
      },
      "exec-train-lstm-vertex-ai-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "train_lstm_vertex_ai_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform' 'google-cloud-storage' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef train_lstm_vertex_ai_op(\n    project_id: str,\n    region: str,\n    gcs_bucket_name: str,\n    params_path: str,\n    pair: str,\n    timeframe: str,\n    output_gcs_prefix: str, # This is the base directory for versioned artifacts\n    vertex_training_image_uri: str, # Esta es la URI de tu imagen runner-lstm\n    vertex_machine_type: str,\n    vertex_accelerator_type: str,\n    vertex_accelerator_count: int,\n    vertex_service_account: str,\n) -> str:\n    import logging\n    import time\n    from datetime import datetime, timezone\n    from google.cloud import aiplatform as gcp_aiplatform\n    from google.cloud import storage as gcp_storage\n\n    # Configurar logger\n    for handler_idx in range(len(logging.root.handlers)): # Bucle seguro para eliminar\n        logging.root.removeHandler(logging.root.handlers[0])\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s [%(funcName)s]: %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    staging_gcs_path = f\"gs://{gcs_bucket_name}/staging_for_custom_jobs\"\n    logger.info(f\"Configurando staging bucket para Vertex AI Custom Job: {staging_gcs_path}\")\n    gcp_aiplatform.init(project=project_id, location=region, staging_bucket=staging_gcs_path)\n    logger.info(f\"Vertex AI SDK inicializado. Project: {project_id}, Region: {region}, Staging Bucket: {staging_gcs_path}\")\n\n    timestamp_for_job = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n    job_display_name = f\"lstm-training-{pair.lower()}-{timeframe.lower()}-{timestamp_for_job}\"\n\n    # Arguments for the train_lstm.py script\n    training_script_args = [\n        \"--params\", params_path,\n        \"--output-gcs-base-dir\", output_gcs_prefix, # Carpeta base donde se guardan los artefactos versionados\n        \"--pair\", pair,                             # S\u00edmbolo de mercado (ej. EURUSD)\n        \"--timeframe\", timeframe,                   # Timeframe (ej. 15minute)\n        \"--project-id\", project_id,                 # ID del proyecto GCP\n        \"--gcs-bucket-name\", gcs_bucket_name        # Bucket principal de almacenamiento\n    ]\n\n    worker_pool_specs = [{\n        \"machine_spec\": {\n            \"machine_type\": vertex_machine_type,\n        },\n        \"replica_count\": 1,\n        \"container_spec\": {\n            \"image_uri\": vertex_training_image_uri,\n            \"args\": training_script_args,\n            # \"command\" se omite para usar el ENTRYPOINT de la imagen runner-lstm\n        },\n    }]\n\n    if vertex_accelerator_count > 0 and vertex_accelerator_type != \"ACCELERATOR_TYPE_UNSPECIFIED\":\n        worker_pool_specs[0][\"machine_spec\"][\"accelerator_type\"] = vertex_accelerator_type\n        worker_pool_specs[0][\"machine_spec\"][\"accelerator_count\"] = vertex_accelerator_count\n        logger.info(f\"Configurando Custom Job con GPU: {vertex_accelerator_count} x {vertex_accelerator_type}\")\n    else:\n        logger.info(f\"Configurando Custom Job solo con CPU.\")\n\n    # The base_output_dir for the Vertex AI CustomJob itself.\n    # The train_lstm.py script will create its own versioned subdirectory inside output_gcs_prefix.\n    vertex_job_base_output_dir = f\"gs://{gcs_bucket_name}/vertex_ai_job_outputs/{job_display_name}\"\n\n    logger.info(f\"Submitting Vertex AI Custom Job: {job_display_name}\")\n    logger.info(f\"  Training script args: {json.dumps(training_script_args)}\")\n    logger.info(f\"  Vertex AI Custom Job base output directory (for job metadata): {vertex_job_base_output_dir}\")\n    logger.info(f\"  LSTM model artifacts base output directory (passed to script): {output_gcs_prefix}\")\n\n\n    custom_job = gcp_aiplatform.CustomJob(\n        display_name=job_display_name,\n        worker_pool_specs=worker_pool_specs,\n        base_output_dir=vertex_job_base_output_dir, # For Vertex AI job artifacts\n        project=project_id,\n        location=region,\n    )\n    custom_job_launch_time_utc = datetime.utcnow().replace(tzinfo=timezone.utc)\n    try:\n        custom_job.run(service_account=vertex_service_account, sync=True, timeout=10800) # 3 horas timeout\n        logger.info(f\"Vertex AI Custom Job {custom_job.display_name} (ID: {custom_job.resource_name}) completed.\")\n    except Exception as e:\n        logger.error(f\"Vertex AI Custom Job {job_display_name} failed or timed out: {e}\")\n        if custom_job and custom_job.resource_name:\n             logger.error(f\"Job details: {custom_job.resource_name}, state: {custom_job.state}\")\n        raise RuntimeError(f\"Vertex AI Custom Job for LSTM training failed: {e}\")\n\n    if not output_gcs_prefix.startswith(\"gs://\"):\n        raise ValueError(\"output_gcs_prefix debe ser una ruta GCS (gs://...)\")\n\n    # The train_lstm.py script is now responsible for creating a timestamped directory\n    # inside output_gcs_prefix. We need to find that directory.\n    # Example: output_gcs_prefix = gs://<bucket>/models/LSTM_v2\n    # train_lstm.py creates: gs://<bucket>/models/LSTM_v2/<pair>/<timeframe>/<timestamp>/\n    # We poll for this <timestamp> directory.\n\n    prefix_parts = output_gcs_prefix.replace(\"gs://\", \"\").split(\"/\")\n    expected_bucket_name = prefix_parts[0]\n    # The listing prefix should be output_gcs_prefix + /<pair>/<timeframe>/\n    gcs_listing_prefix = f\"{'/'.join(prefix_parts[1:])}/{pair}/{timeframe}/\"\n    if not gcs_listing_prefix.endswith('/'): gcs_listing_prefix += '/'\n\n    logger.info(f\"Polling GCS bucket '{expected_bucket_name}' under prefix '{gcs_listing_prefix}' for model artifacts directory created by the training script (after {custom_job_launch_time_utc.isoformat()}).\")\n\n    storage_client = gcp_storage.Client(project=project_id)\n    bucket = storage_client.bucket(expected_bucket_name)\n    max_poll_wait_sec, poll_interval_sec = 15 * 60, 30 # 15 minutes\n    start_poll_time = time.time()\n    found_model_dir_gcs_path = None\n\n    while time.time() - start_poll_time < max_poll_wait_sec:\n        logger.info(f\"GCS poll elapsed: {int(time.time() - start_poll_time)}s / {max_poll_wait_sec}s. Checking prefix: gs://{expected_bucket_name}/{gcs_listing_prefix}\")\n        blobs = bucket.list_blobs(prefix=gcs_listing_prefix, delimiter=\"/\") # delimiter lists \"subdirectories\"\n        candidate_dirs = []\n        for page in blobs.pages: # Iterate through pages of results\n            if hasattr(page, 'prefixes') and page.prefixes: # page.prefixes contains the \"subdirectories\"\n                for dir_prefix_full_path in page.prefixes: # e.g., models/LSTM_v2/EURUSD/15minute/20230101120000/\n                    # Check if model.h5 exists within this subdirectory\n                    model_blob_path = f\"{dir_prefix_full_path.rstrip('/')}/model.h5\" # Path within the bucket\n                    if bucket.blob(model_blob_path).exists():\n                        # Verify the directory name is a timestamp (format %Y%m%d%H%M%S)\n                        try:\n                            dir_name_part = dir_prefix_full_path.rstrip('/').split('/')[-1]\n                            datetime.strptime(dir_name_part, \"%Y%m%d%H%M%S\") # Validate format\n                            candidate_dirs.append(f\"gs://{expected_bucket_name}/{dir_prefix_full_path.rstrip('/')}\")\n                        except ValueError:\n                            logger.warning(f\"Directory {dir_prefix_full_path} does not seem to be a timestamped model directory. Skipping.\")\n                            continue\n        if candidate_dirs:\n            # If multiple timestamped directories are found (e.g., from previous runs or quick retries),\n            # pick the latest one. The train_lstm.py script creates a new timestamped dir each time.\n            found_model_dir_gcs_path = sorted(candidate_dirs, reverse=True)[0]\n            logger.info(f\"Found LSTM model directory in GCS: {found_model_dir_gcs_path}\")\n            return found_model_dir_gcs_path # Return the GCS path to the versioned model directory\n        time.sleep(poll_interval_sec)\n\n    err_msg = f\"LSTM model directory (containing model.h5) not found in gs://{expected_bucket_name}/{gcs_listing_prefix} after {max_poll_wait_sec / 60:.1f} min. Vertex Job: {custom_job.display_name}\"\n    logger.error(err_msg)\n    raise TimeoutError(err_msg)\n\n"
          ],
          "image": "python:3.9-slim",
          "resources": {
            "cpuLimit": 1.0,
            "memoryLimit": 2.0,
            "resourceCpuLimit": "1",
            "resourceMemoryLimit": "2G"
          }
        }
      },
      "exec-train-rl-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "train_rl_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'gcsfs' 'pandas' 'numpy' 'tensorflow' 'stable-baselines3[extra]' 'gymnasium' 'pandas_ta' 'scikit-learn' 'joblib' 'optuna' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef train_rl_op(\n    gcs_bucket_name: str,\n    lstm_model_dir: str, # This is the versioned GCS path from train_lstm_vertex_ai_op\n    rl_data_path: str,   # This is the versioned GCS path from prepare_rl_data_op\n    pair: str,\n    timeframe: str,\n    output_gcs_prefix: str, # Base GCS path for RL model outputs\n    tensorboard_logs_gcs_prefix: str,\n    project_id: str,\n) -> str: # Returns the GCS path to the trained RL model zip file\n    import subprocess\n    from datetime import datetime\n    import logging\n    from pathlib import Path\n\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    if not lstm_model_dir or not lstm_model_dir.startswith(\"gs://\"):\n        raise ValueError(f\"train_rl_op requiere una ruta GCS v\u00e1lida para lstm_model_dir, se obtuvo: {lstm_model_dir}\")\n\n    # lstm_params_path should point to the params.json COPIED by train_lstm.py into its output directory\n    lstm_params_path = f\"{lstm_model_dir.rstrip('/')}/params.json\"\n    tensorboard_log_gcs_path = f\"{tensorboard_logs_gcs_prefix.rstrip('/')}/{pair}/{timeframe}/{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\"\n\n    logger.info(f\"Initializing train_rl.py for {pair} {timeframe}. LSTM params (from LSTM artifact dir): {lstm_params_path}, RL Data: {rl_data_path}, Output Base: {output_gcs_prefix}, TensorBoard: {tensorboard_log_gcs_path}\")\n    command = [\n        \"python\", \"train_rl.py\",\n        \"--params\", lstm_params_path, # This is the params.json from the LSTM model artifact directory\n        \"--rl-data\", rl_data_path,\n        \"--output-bucket\", gcs_bucket_name, # Bucket for train_rl.py to write to\n        \"--pair\", pair,\n        \"--timeframe\", timeframe,\n        \"--output-model-base-gcs-path\", output_gcs_prefix, # train_rl.py will create versioned subdirs here\n        \"--tensorboard-log-dir\", tensorboard_log_gcs_path,\n    ]\n    logger.info(f\"Executing command: {' '.join(command)}\")\n    process = subprocess.run(command, capture_output=True, text=True, check=False)\n\n    if process.returncode != 0:\n        error_msg = f\"Error in train_rl.py for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\\nSTDERR: {process.stderr}\"\n        logger.error(error_msg)\n        raise RuntimeError(error_msg)\n    else:\n        logger.info(f\"train_rl.py completed for {pair} {timeframe}.\\nSTDOUT: {process.stdout}\")\n        # train_rl.py should print the GCS path of the saved model.\n        # Example expected output line: \"RL model saved to gs://<bucket>/models/RL_v2/<pair>/<timeframe>/<timestamp>/ppo_filter_model.zip\"\n        rl_model_upload_line = next((l for l in process.stdout.splitlines() if \"RL model saved to gs://\" in l or \"Subido ppo_filter_model.zip a\" in l), None)\n        if rl_model_upload_line:\n            if \"RL model saved to \" in rl_model_upload_line:\n                 uploaded_rl_path = rl_model_upload_line.split(\"RL model saved to \")[1].strip()\n            elif \"Subido ppo_filter_model.zip a \" in rl_model_upload_line: # Legacy or alternative phrasing\n                 uploaded_rl_path = rl_model_upload_line.split(\"Subido ppo_filter_model.zip a \")[1].strip()\n            else: # Fallback if the exact phrase isn't matched but a gs:// path is there\n                logger.warning(\"Could not precisely determine RL model path from stdout using known patterns. Looking for any 'gs://' path ending in .zip.\")\n                gs_paths_in_stdout = [word for word in process.stdout.split() if word.startswith(\"gs://\") and word.endswith(\".zip\")]\n                if gs_paths_in_stdout:\n                    uploaded_rl_path = gs_paths_in_stdout[-1] # Assume the last one is the relevant one\n                    logger.info(f\"Guessed RL model path from stdout: {uploaded_rl_path}\")\n                else:\n                    raise RuntimeError(\"Failed to determine RL model output path from train_rl.py stdout. No 'gs://...zip' path found.\")\n            logger.info(f\"RL model uploaded to: {uploaded_rl_path}\")\n            return uploaded_rl_path.rstrip('/') # Return the full GCS path to the .zip file\n        else:\n            raise RuntimeError(\"Failed to determine RL model output path from train_rl.py stdout. Ensure the script prints the GCS path like 'RL model saved to gs://...'.\")\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest",
          "resources": {
            "cpuLimit": 16.0,
            "memoryLimit": 60.0,
            "resourceCpuLimit": "16",
            "resourceMemoryLimit": "60G"
          }
        }
      },
      "exec-update-data-op": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "update_data_op"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage' 'google-cloud-pubsub' 'gcsfs' 'pandas' 'google-cloud-secret-manager' 'requests' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\n\ndef update_data_op(\n    pair: str,\n    timeframe: str,\n    gcs_bucket_name: str,\n    project_id: str,\n    polygon_api_key_secret_name: str,\n    polygon_api_key_secret_version: str = \"latest\",\n) -> str:\n    import subprocess\n    import logging\n    import json\n    import os\n    import requests\n    from google.cloud import secretmanager as sm_client_lib\n\n    # Configurar logger\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s [%(funcName)s]: %(message)s\")\n    logger = logging.getLogger(__name__)\n\n    try:\n        metadata_url = \"http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/email\"\n        headers = {\"Metadata-Flavor\": \"Google\"}\n        sa_email_response = requests.get(metadata_url, headers=headers, timeout=5)\n        sa_email_response.raise_for_status()\n        current_sa = sa_email_response.text\n        logger.info(f\"El componente update-data-op se est\u00e1 ejecutando con la cuenta de servicio (desde metadata): {current_sa}\")\n    except Exception as sa_err:\n        logger.warning(f\"No se pudo obtener la cuenta de servicio actual del metadata server: {sa_err}\")\n        logger.warning(\"Esto es normal si se ejecuta localmente fuera de GCP. En GCP, podr\u00eda indicar un problema de red/configuraci\u00f3n del metadata server.\")\n\n    try:\n        logger.info(f\"Accediendo al secreto: projects/{project_id}/secrets/{polygon_api_key_secret_name}/versions/{polygon_api_key_secret_version}\")\n        client = sm_client_lib.SecretManagerServiceClient()\n        secret_path = client.secret_version_path(\n            project_id,\n            polygon_api_key_secret_name,\n            polygon_api_key_secret_version\n        )\n        response = client.access_secret_version(request={\"name\": secret_path})\n        polygon_api_key = response.payload.data.decode(\"UTF-8\")\n        os.environ[\"POLYGON_API_KEY\"] = polygon_api_key\n        logger.info(f\"API Key de Polygon obtenida de Secret Manager ({polygon_api_key_secret_name}) y configurada en el entorno.\")\n    except Exception as e:\n        logger.error(f\"Error al obtener la API Key de Polygon de Secret Manager: {e}\", exc_info=True)\n        raise RuntimeError(f\"Fallo al obtener la API Key de Secret Manager '{polygon_api_key_secret_name}': {e}\")\n\n    logger.info(f\"Initializing data_orchestrator.py for {pair} {timeframe}...\")\n    message_dict = {\"symbols\": [pair], \"timeframes\": [timeframe]}\n    message_str = json.dumps(message_dict)\n\n    command = [\n        \"python\", \"data_orchestrator.py\",\n        \"--mode\", \"on-demand\",\n        \"--message\", message_str,\n        \"--project_id\", project_id\n    ]\n    logger.info(f\"Command to execute for data_orchestrator.py: {' '.join(command)}\")\n\n    current_env = os.environ.copy()\n    process = subprocess.run(command, capture_output=True, text=True, check=False, env=current_env)\n\n    if process.returncode != 0:\n        error_message = f\"data_orchestrator.py failed for {pair} {timeframe}.\"\n        stdout_output = process.stdout.strip()\n        stderr_output = process.stderr.strip()\n        if stdout_output: error_message += f\"\\nSTDOUT:\\n{stdout_output}\"\n        if stderr_output: error_message += f\"\\nSTDERR:\\n{stderr_output}\"\n        logger.error(error_message)\n        raise RuntimeError(error_message)\n    else:\n        logger.info(f\"data_orchestrator.py completed for {pair} {timeframe}.\\nSTDOUT: {process.stdout.strip()}\")\n        if process.stderr.strip():\n            logger.info(f\"STDERR (aunque exit code fue 0):\\n{process.stderr.strip()}\")\n    return f\"Data update process completed for {pair}/{timeframe}.\"\n\n"
          ],
          "image": "europe-west1-docker.pkg.dev/trading-ai-460823/data-ingestion-repo/data-ingestion-agent:latest",
          "resources": {
            "cpuLimit": 2.0,
            "memoryLimit": 4.0,
            "resourceCpuLimit": "2",
            "resourceMemoryLimit": "4G"
          }
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "KFP v2 Pipeline for training and deploying algorithmic trading models.",
    "name": "algo-trading-mlops-gcp-pipeline-v2"
  },
  "root": {
    "dag": {
      "tasks": {
        "exit-handler-1": {
          "componentRef": {
            "name": "comp-exit-handler-1"
          },
          "inputs": {
            "parameters": {
              "pipelinechannel--backtest_features_path": {
                "componentInputParameter": "backtest_features_path"
              },
              "pipelinechannel--n_trials": {
                "componentInputParameter": "n_trials"
              },
              "pipelinechannel--pair": {
                "componentInputParameter": "pair"
              },
              "pipelinechannel--polygon_api_key_secret_name_param": {
                "componentInputParameter": "polygon_api_key_secret_name_param"
              },
              "pipelinechannel--polygon_api_key_secret_version_param": {
                "componentInputParameter": "polygon_api_key_secret_version_param"
              },
              "pipelinechannel--timeframe": {
                "componentInputParameter": "timeframe"
              },
              "pipelinechannel--vertex_lstm_accelerator_count": {
                "componentInputParameter": "vertex_lstm_accelerator_count"
              },
              "pipelinechannel--vertex_lstm_accelerator_type": {
                "componentInputParameter": "vertex_lstm_accelerator_type"
              },
              "pipelinechannel--vertex_lstm_machine_type": {
                "componentInputParameter": "vertex_lstm_machine_type"
              },
              "pipelinechannel--vertex_lstm_service_account": {
                "componentInputParameter": "vertex_lstm_service_account"
              },
              "pipelinechannel--vertex_lstm_training_image": {
                "componentInputParameter": "vertex_lstm_training_image"
              }
            }
          },
          "taskInfo": {
            "name": "exit-handler-1"
          }
        },
        "notify-pipeline-status-op": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-notify-pipeline-status-op"
          },
          "dependentTasks": [
            "exit-handler-1"
          ],
          "inputs": {
            "parameters": {
              "model_promoted": {
                "runtimeValue": {
                  "constant": false
                }
              },
              "pair": {
                "componentInputParameter": "pair"
              },
              "pipeline_job_id": {
                "runtimeValue": {
                  "constant": "{{$.pipeline_job_uuid}}"
                }
              },
              "pipeline_job_link": {
                "runtimeValue": {
                  "constant": "https://console.cloud.google.com/vertex-ai/pipelines/runs/{{$.pipeline_job_uuid}}?project=trading-ai-460823"
                }
              },
              "pipeline_run_status": {
                "runtimeValue": {
                  "constant": "UNKNOWN_STATUS"
                }
              },
              "project_id": {
                "runtimeValue": {
                  "constant": "trading-ai-460823"
                }
              },
              "timeframe": {
                "componentInputParameter": "timeframe"
              }
            }
          },
          "taskInfo": {
            "name": "Notify_Pipeline_Final_Status_Exit_Handler"
          },
          "triggerPolicy": {
            "strategy": "ALL_UPSTREAM_TASKS_COMPLETED"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "backtest_features_path": {
          "defaultValue": "gs://trading-ai-models-460823/backtest_data/EURUSD_15minute_unseen.parquet",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "kfp_opt_accelerator_count": {
          "defaultValue": 0.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "kfp_opt_accelerator_type": {
          "defaultValue": "ACCELERATOR_TYPE_UNSPECIFIED",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "n_trials": {
          "defaultValue": 2.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "pair": {
          "defaultValue": "EURUSD",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "polygon_api_key_secret_name_param": {
          "defaultValue": "polygon-api-key",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "polygon_api_key_secret_version_param": {
          "defaultValue": "latest",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "timeframe": {
          "defaultValue": "15minute",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "vertex_lstm_accelerator_count": {
          "defaultValue": 0.0,
          "isOptional": true,
          "parameterType": "NUMBER_INTEGER"
        },
        "vertex_lstm_accelerator_type": {
          "defaultValue": "ACCELERATOR_TYPE_UNSPECIFIED",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "vertex_lstm_machine_type": {
          "defaultValue": "n1-standard-4",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "vertex_lstm_service_account": {
          "defaultValue": "data-ingestion-agent@trading-ai-460823.iam.gserviceaccount.com",
          "isOptional": true,
          "parameterType": "STRING"
        },
        "vertex_lstm_training_image": {
          "defaultValue": "us-central1-docker.pkg.dev/trading-ai-460823/trading-images/runner-lstm:latest",
          "isOptional": true,
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.13.0"
}