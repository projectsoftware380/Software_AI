#!/usr/bin/env python3
"""
prepare_opt_data.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Genera el subconjunto de datos histÃ³ricos que se usarÃ¡ para la bÃºsqueda de
hiperparÃ¡metros (â€œoptimizaciÃ³nâ€) de los modelos LSTM.

â€¢ Lee un Parquet OHLC completo â€•local o gs://â€•.
â€¢ Garantiza que las columnas esenciales se llamen: open, high, low, close,
  volume (opcional) y timestamp (datetime UTC).
â€¢ Elimina filas con NaNs en OHLC.
â€¢ Filtra los Ãºltimos N aÃ±os (por defecto 5).
â€¢ Guarda el resultado en la ruta de salida (local o gs://).

Ejemplo CLI
-----------
python prepare_opt_data.py \
  --input_path  gs://bucket/data/EURUSD_15minute.parquet \
  --output_path gs://bucket/data_filtered_for_opt/EURUSD_15minute_recent.parquet \
  --years 5
"""

from __future__ import annotations
import argparse
import logging
import os
import sys
import tempfile
from datetime import datetime
from pathlib import Path

import pandas as pd

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
)
log = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GCS helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    import gcsfs
    from google.cloud import storage
    from google.auth.exceptions import DefaultCredentialsError
except ImportError:  # fallback defensivo si faltan deps (no deberÃ­a ocurrir en la imagen)
    import subprocess

    subprocess.check_call(
        [sys.executable, "-m", "pip", "install", "-q", "gcsfs", "google-cloud-storage"]
    )
    import gcsfs
    from google.cloud import storage
    from google.auth.exceptions import DefaultCredentialsError


def _gcs_client() -> storage.Client:
    try:
        return storage.Client()  # usa ADC (cuenta de servicio) o cred locales
    except DefaultCredentialsError as e:
        log.critical(
            "âŒ No fue posible obtener credenciales de GCP. "
            "AsegÃºrate de que GOOGLE_APPLICATION_CREDENTIALS estÃ© definida "
            "o de ejecutar `gcloud auth application-default login` en local."
        )
        raise e


def _download_gs(uri: str) -> Path:
    if not uri.startswith("gs://"):
        raise ValueError("SÃ³lo se aceptan URIs que empiecen con gs://")
    bucket_name, blob_name = uri[5:].split("/", 1)
    local = Path(tempfile.mkdtemp()) / Path(blob_name).name
    _gcs_client().bucket(bucket_name).blob(blob_name).download_to_filename(local)
    log.info(f"ğŸ“¥  Descargado {uri}  â†’  {local}")
    return local


def _upload_gs(local: Path, uri: str):
    if not uri.startswith("gs://"):
        raise ValueError("La ruta de salida debe empezar por gs://")
    bucket_name, blob_name = uri[5:].split("/", 1)
    _gcs_client().bucket(bucket_name).blob(blob_name).upload_from_filename(str(local))
    log.info(f"â˜ï¸  Subido {local}  â†’  {uri}")


def _read_parquet(path: str) -> pd.DataFrame:
    """Lee un Parquet desde local o GCS con pyarrow."""
    if path.startswith("gs://"):
        # gcsfs usa â€˜token="cloud"â€™ para ADC / cred. de la VM
        return pd.read_parquet(path, engine="pyarrow", storage_options={"token": "cloud"})
    return pd.read_parquet(path, engine="pyarrow")


def _write_parquet(df: pd.DataFrame, path: str):
    """Guarda Parquet a local o GCS (mantiene compresiÃ³n por defecto)."""
    if path.startswith("gs://"):
        df.to_parquet(path, index=False, engine="pyarrow", storage_options={"token": "cloud"})
    else:
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        df.to_parquet(path, index=False, engine="pyarrow")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main logic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ESSENTIAL = ["open", "high", "low", "close", "timestamp"]  # volume opcional


def main():
    ap = argparse.ArgumentParser(description="Filtra Ãºltimos N aÃ±os de datos OHLC.")
    ap.add_argument("--input_path", required=True, help="Parquet completo (.parquet o gs://)")
    ap.add_argument("--output_path", required=True, help="Ruta destino (.parquet o gs://)")
    ap.add_argument("--years", type=int, default=5, help="Ventana de aÃ±os a conservar (default 5)")
    args = ap.parse_args()

    log.info(f"âš™ï¸  Iniciando prepare_opt_data (window {args.years} aÃ±os)")

    # 1) Carga
    df = _read_parquet(args.input_path)
    log.info(f"âœ”  Dataset cargado: {len(df):,} filas, {df.columns.tolist()} columnas")

    # 2) Asegurar nombres estÃ¡ndar
    rename_map = {"o": "open", "h": "high", "l": "low", "c": "close", "t": "timestamp"}
    missing = []
    for old, new in rename_map.items():
        if old in df.columns and new not in df.columns:
            df = df.rename(columns={old: new})
    for col in ESSENTIAL:
        if col not in df.columns:
            missing.append(col)
    if missing:
        raise ValueError(f"Columnas esenciales ausentes despuÃ©s del rename: {missing}")

    # 3) Timestamp â†’ datetime UTC
    if not pd.api.types.is_datetime64_any_dtype(df["timestamp"]):
        # Polygon entrega milisegundos â†’ unit="ms"
        df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", errors="coerce")
    # uniformizar a UTC
    if df["timestamp"].dt.tz is None:
        df["timestamp"] = df["timestamp"].dt.tz_localize("UTC")
    else:
        df["timestamp"] = df["timestamp"].dt.tz_convert("UTC")

    # 4) Drop NaNs en OHLC
    df = df.dropna(subset=["open", "high", "low", "close"])
    if df.empty:
        raise ValueError("Todos los registros contienen NaNs en OHLC; abortando.")

    # 5) Filtro temporal
    end_ts = df["timestamp"].max()
    start_ts = end_ts - pd.DateOffset(years=args.years)
    df_filtered = df[df["timestamp"] >= start_ts].copy()
    log.info(
        f"ğŸ—‚  Filtrado desde {start_ts.date()} hasta {end_ts.date()} â‡’ "
        f"{len(df_filtered):,}/{len(df):,} filas"
    )

    if df_filtered.empty:
        raise ValueError("El filtro temporal devolviÃ³ 0 filas. Revisa las fechas / datos.")

    # 6) Guardar
    _write_parquet(df_filtered, args.output_path)
    log.info(f"âœ…  Parquet filtrado guardado en {args.output_path}")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log.critical(f"âŒ  prepare_opt_data terminÃ³ con error: {e}", exc_info=True)
        sys.exit(1)
